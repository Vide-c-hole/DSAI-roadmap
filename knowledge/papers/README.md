# Key Papers

This directory contains notes and summaries of influential AI papers.

## Reading Order (Recommended)

### Foundation (Start Here)
1. **Attention Is All You Need** (2017) - The Transformer paper
2. **BERT** (2018) - Bidirectional pre-training
3. **GPT-2** (2019) - Scaling language models

### Scaling Era
4. **GPT-3** (2020) - Few-shot learning
5. **Scaling Laws** (2020) - Predictable scaling
6. **Chinchilla** (2022) - Compute-optimal training

### Alignment & Instruction
7. **InstructGPT** (2022) - RLHF
8. **Constitutional AI** (2022) - AI safety
9. **DPO** (2023) - Direct preference optimization

### Agents
10. **Chain-of-Thought** (2022) - Reasoning
11. **ReAct** (2023) - Reasoning + Acting
12. **Tree of Thoughts** (2023) - Complex reasoning
13. **Generative Agents** (2023) - Believable simulations

### Efficiency
14. **LoRA** (2021) - Efficient fine-tuning
15. **Mixtral** (2024) - Mixture of Experts

## Paper Template

For each paper, create a markdown file with:

```markdown
# [Paper Title]

**Authors:**
**Year:**
**Link:**

## Key Contribution
[1-2 sentences]

## Main Ideas
-
-
-

## Architecture/Method
[Diagram or description]

## Results
[Key findings]

## My Notes
[Personal understanding and questions]

## Code
[Link to implementation if available]
```

## Papers by Category

### Attention & Transformers
- [ ] Attention Is All You Need
- [ ] BERT
- [ ] GPT series

### Training & Alignment
- [ ] InstructGPT (RLHF)
- [ ] Constitutional AI
- [ ] DPO

### Efficiency
- [ ] LoRA
- [ ] QLoRA
- [ ] Mixture of Experts

### Agents
- [ ] ReAct
- [ ] Toolformer
- [ ] Generative Agents

### Multimodal
- [ ] CLIP
- [ ] GPT-4V
- [ ] LLaVA
